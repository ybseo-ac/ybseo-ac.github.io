<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Hi! I am a second-year M.S. student at Yonsei University. I am currently at the ML3 lab of professor <a href="https://sites.google.com/view/jaehyungkim" rel="external nofollow noopener" target="_blank">Jaehyung Kim</a> for a research exchange. I was previously advised by professor <a href="https://jinyeo.weebly.com/" rel="external nofollow noopener" target="_blank">Jinyoung Yeo</a>. I received B.S. in Educational Science from Yonsei University. I also served as a journalist specializing in data analysis in <a href="https://www.donga.com/" rel="external nofollow noopener" target="_blank">Donga Ilbo</a>, which is one of the top major newspapers in South Korea.</p> <h4 id="research-interest--ml-nlp">Research Interest (ML, NLP)</h4> <p>My research is focused on building human-like self-evolving AI, and I believe this requires improving the architecture of LMs. For example, to accelerate RL in LMs, I believe the LMs need goal-orientation (bidirectional attention) and faster generation speed, which motivated my research on <strong>diffusion LMs</strong>. I also believe that, for RL of LMs, the token-level granularity of LMs should be compressed into concept-level units in order to avoid the long-horizon (i.e., sparse reward) problem, so I’m currently working on <strong>hierarchical LMs</strong> (i.e., <strong>latent reasoning</strong>). \</p> <p>Also, I conducted research on <strong>continual knowledge learning</strong>, which is essential in self-evolving AI. I also explored <strong>efficient learning via data pruning</strong>, which will be required for continual learning of a vast stream of text data. I am potentially open to research on <strong>embodied agent</strong>, to build the <strong>emotion and personality of LM</strong>.</p> </body></html>